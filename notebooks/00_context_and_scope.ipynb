{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eadca03",
   "metadata": {},
   "source": [
    "<img src=\"../img/f1pa_banner.png\" alt=\"F1PA banner\" height=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232dd32",
   "metadata": {},
   "source": [
    "# Formula 1 Predictive Assistant (F1PA)\n",
    "\n",
    "Ce projet a pour objectif de concevoir un **assistant prédictif appliqué à la Formule 1**, capable d’estimer le **temps au tour potentiel** d’un pilote dans un contexte donné (circuit, conditions météo, session).\n",
    "\n",
    "Le projet est réalisé dans un cadre **pédagogique et de certification Data / IA**, avec une attention particulière portée à :\n",
    "- la structuration des données,\n",
    "- la reproductibilité des traitements,\n",
    "- la traçabilité des décisions techniques et méthodologiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b6bac",
   "metadata": {},
   "source": [
    "## Contexte et périmètre\n",
    "\n",
    "Le projet couvre les saisons de Formule 1 **2022 à 2024**, avec une **extension possible à 2025** selon la disponibilité et la qualité des données.\n",
    "\n",
    "Il ne s’agit pas d’un système de prédiction temps réel en production, mais d’un **cas d’étude complet**, destiné à démontrer des compétences professionnelles en :\n",
    "- **Data Engineering** (ETL, orchestration, bases de données),\n",
    "- **Machine Learning** (feature engineering, entraînement, évaluation),\n",
    "- **MLOps** (tracking des expériences, monitoring, tests)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f789ce6",
   "metadata": {},
   "source": [
    "## Problématique data et machine learning\n",
    "\n",
    "La problématique centrale du projet est la suivante :\n",
    "\n",
    "> *Comment estimer un temps au tour en Formule 1 à partir de données historiques, en tenant compte du circuit et des conditions environnementales ?*\n",
    "\n",
    "Ce problème est formulé comme une **tâche de régression supervisée**, où la variable cible est le **temps au tour**, exprimé en millisecondes.\n",
    "\n",
    "L’objectif n’est pas d’obtenir une prédiction parfaite, mais de construire un pipeline cohérent, explicable et reproductible, du jeu de données brut jusqu’au modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc8b982",
   "metadata": {},
   "source": [
    "## Sources de données\n",
    "\n",
    "Le projet repose sur trois sources de données distinctes et complémentaires :\n",
    "\n",
    "1. **OpenF1 API**\n",
    "   - Données sportives historiques\n",
    "   - Structure des saisons, sessions, pilotes et temps au tour\n",
    "\n",
    "2. **Meteostat**\n",
    "   - Données météorologiques historiques\n",
    "   - Température, précipitations, vent, pression atmosphérique\n",
    "\n",
    "3. **Wikipedia**\n",
    "   - Données descriptives des circuits\n",
    "   - Localisation, caractéristiques générales, contexte géographique\n",
    "\n",
    "Chaque source joue un rôle spécifique dans la compréhension et l’enrichissement des performances en piste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306dd7e",
   "metadata": {},
   "source": [
    "## Granularité des données\n",
    "\n",
    "La granularité cible finale du projet est la suivante :\n",
    "\n",
    "> **1 ligne = 1 tour chronométré d’un pilote lors d’une session donnée**\n",
    "\n",
    "Cette granularité permet :\n",
    "- une analyse fine des performances individuelles,\n",
    "- un feature engineering riche et pertinent,\n",
    "- une approche Machine Learning réaliste et exploitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb3369",
   "metadata": {},
   "source": [
    "## Architecture logique des données (ETL)\n",
    "\n",
    "Le projet adopte une approche **ETL (Extract – Transform – Load)**, volontairement simple et progressive.\n",
    "\n",
    "### Extract\n",
    "- Récupération des données depuis les différentes sources :\n",
    "  - OpenF1 (API)\n",
    "  - Meteostat (fichiers CSV)\n",
    "  - Wikipedia (scraping)\n",
    "- Les données extraites sont conservées sous forme de fichiers afin de garantir la reproductibilité.\n",
    "\n",
    "### Transform\n",
    "- Nettoyage, normalisation et enrichissement des données :\n",
    "  - harmonisation des schémas,\n",
    "  - jointures entre sources (circuits, sessions, météo),\n",
    "  - préparation des variables explicatives.\n",
    "\n",
    "### Load\n",
    "- Chargement des données transformées dans une **base PostgreSQL locale**\n",
    "- Mise à disposition des datasets pour :\n",
    "  - l’entraînement des modèles ML,\n",
    "  - l’API et l’application de visualisation.\n",
    "\n",
    "Cette architecture vise la lisibilité, la traçabilité et l’alignement avec les compétences attendues en Data Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e9a736",
   "metadata": {},
   "source": [
    "## Stack technique et contraintes\n",
    "\n",
    "- Python : **3.10.10**\n",
    "- Base de données : **PostgreSQL (local)**\n",
    "- Orchestration : **Apache Airflow (LocalExecutor)**\n",
    "- Suivi des modèles : **MLflow (local)**\n",
    "- API : **FastAPI**\n",
    "- Visualisation : **Streamlit**\n",
    "- Conteneurisation : **Docker / Docker Compose**\n",
    "\n",
    "Tous les services du projet sont exécutés **localement** et conteneurisés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f17c05",
   "metadata": {},
   "source": [
    "## Rôle des notebooks\n",
    "\n",
    "Les notebooks Jupyter sont utilisés exclusivement pour :\n",
    "- l’exploration et la compréhension des sources de données,\n",
    "- la validation des schémas et hypothèses,\n",
    "- le prototypage exploratoire.\n",
    "\n",
    "Les traitements destinés à être rejoués ou orchestrés sont implémentés sous forme de **scripts Python industrialisables**, exécutés via Airflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274fa949",
   "metadata": {},
   "source": [
    "## Étapes suivantes\n",
    "\n",
    "Les prochaines étapes du projet sont :\n",
    "\n",
    "1. Exploration détaillée des sources de données (OpenF1, Meteostat, Wikipedia)\n",
    "2. Stabilisation des données extraites (fichiers Extract)\n",
    "3. Définition des transformations nécessaires à l’enrichissement des données\n",
    "4. Implémentation progressive des pipelines ETL via Airflow\n",
    "5. Chargement des données dans PostgreSQL\n",
    "6. Entraînement, suivi et évaluation des modèles de Machine Learning"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
